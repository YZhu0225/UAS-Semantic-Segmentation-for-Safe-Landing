{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# MobileNet"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T20:21:19.454249Z","iopub.status.busy":"2023-04-09T20:21:19.453098Z","iopub.status.idle":"2023-04-09T22:01:51.053398Z","shell.execute_reply":"2023-04-09T22:01:51.052004Z","shell.execute_reply.started":"2023-04-09T20:21:19.454212Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b7688ef39e9472d8604921af03dae19","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n","/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/200], Train Loss: 1.7683, Train Acc: 0.3296, Test Loss: 2.1071, Test Acc: 0.2367\n","Epoch [2/200], Train Loss: 1.5482, Train Acc: 0.4267, Test Loss: 2.0478, Test Acc: 0.3681\n","Epoch [3/200], Train Loss: 1.4657, Train Acc: 0.4664, Test Loss: 1.5436, Test Acc: 0.4461\n","Epoch [4/200], Train Loss: 1.4351, Train Acc: 0.4767, Test Loss: 1.7409, Test Acc: 0.3911\n","Epoch [5/200], Train Loss: 1.4071, Train Acc: 0.4871, Test Loss: 1.3712, Test Acc: 0.5000\n","Epoch [6/200], Train Loss: 1.5535, Train Acc: 0.4284, Test Loss: 1.4708, Test Acc: 0.4581\n","Epoch [7/200], Train Loss: 1.7065, Train Acc: 0.3699, Test Loss: 3.2442, Test Acc: 0.1830\n","Epoch [8/200], Train Loss: 1.5843, Train Acc: 0.4141, Test Loss: 1.4705, Test Acc: 0.4496\n","Epoch [9/200], Train Loss: 1.4509, Train Acc: 0.4675, Test Loss: 1.4435, Test Acc: 0.4668\n","Epoch [10/200], Train Loss: 1.4947, Train Acc: 0.4513, Test Loss: 1.8074, Test Acc: 0.3662\n","Epoch [11/200], Train Loss: 1.4303, Train Acc: 0.4755, Test Loss: 1.3365, Test Acc: 0.5100\n","Epoch [12/200], Train Loss: 1.3367, Train Acc: 0.5131, Test Loss: 1.2440, Test Acc: 0.5491\n","Epoch [13/200], Train Loss: 1.2226, Train Acc: 0.5572, Test Loss: 1.2464, Test Acc: 0.5518\n","Epoch [14/200], Train Loss: 1.1947, Train Acc: 0.5691, Test Loss: 808.4341, Test Acc: 0.1179\n","Epoch [15/200], Train Loss: 1.4169, Train Acc: 0.4816, Test Loss: 1.4989, Test Acc: 0.4723\n","Epoch [16/200], Train Loss: 1.3684, Train Acc: 0.5024, Test Loss: 1.4489, Test Acc: 0.4761\n","Epoch [17/200], Train Loss: 1.3394, Train Acc: 0.5133, Test Loss: 3.1150, Test Acc: 0.2000\n","Epoch [18/200], Train Loss: 1.3408, Train Acc: 0.5159, Test Loss: 1.3239, Test Acc: 0.5294\n","Epoch [19/200], Train Loss: 1.4117, Train Acc: 0.4893, Test Loss: 2.5480, Test Acc: 0.2001\n","Epoch [20/200], Train Loss: 1.5818, Train Acc: 0.4268, Test Loss: 1.5647, Test Acc: 0.4351\n","Epoch [21/200], Train Loss: 1.3982, Train Acc: 0.4904, Test Loss: 1.7039, Test Acc: 0.4243\n","Epoch [22/200], Train Loss: 1.3124, Train Acc: 0.5261, Test Loss: 2.4992, Test Acc: 0.2698\n","Epoch [23/200], Train Loss: 1.2867, Train Acc: 0.5361, Test Loss: 8.6771, Test Acc: 0.2872\n","Epoch [24/200], Train Loss: 1.2346, Train Acc: 0.5545, Test Loss: 1.2293, Test Acc: 0.5587\n","Epoch [25/200], Train Loss: 1.4261, Train Acc: 0.4809, Test Loss: 2.1693, Test Acc: 0.3293\n","Epoch [26/200], Train Loss: 1.2715, Train Acc: 0.5408, Test Loss: 1.1918, Test Acc: 0.5690\n","Epoch [27/200], Train Loss: 1.1928, Train Acc: 0.5723, Test Loss: 1.1279, Test Acc: 0.5986\n","Epoch [28/200], Train Loss: 1.1016, Train Acc: 0.6073, Test Loss: 1.1387, Test Acc: 0.5955\n","Epoch [29/200], Train Loss: 1.0541, Train Acc: 0.6225, Test Loss: 1.0436, Test Acc: 0.6288\n","Epoch [30/200], Train Loss: 1.0267, Train Acc: 0.6337, Test Loss: 0.9947, Test Acc: 0.6457\n","Epoch [31/200], Train Loss: 1.0044, Train Acc: 0.6399, Test Loss: 0.9995, Test Acc: 0.6482\n","Epoch [32/200], Train Loss: 0.9805, Train Acc: 0.6518, Test Loss: 1.0308, Test Acc: 0.6305\n","Epoch [33/200], Train Loss: 0.9646, Train Acc: 0.6552, Test Loss: 1.0153, Test Acc: 0.6443\n","Epoch [34/200], Train Loss: 0.9448, Train Acc: 0.6638, Test Loss: 1.1474, Test Acc: 0.5901\n","Epoch [35/200], Train Loss: 0.9342, Train Acc: 0.6670, Test Loss: 0.9472, Test Acc: 0.6689\n","Epoch [36/200], Train Loss: 0.9138, Train Acc: 0.6760, Test Loss: 1.0493, Test Acc: 0.6316\n","Epoch [37/200], Train Loss: 0.9012, Train Acc: 0.6803, Test Loss: 1.0025, Test Acc: 0.6506\n","Epoch [38/200], Train Loss: 0.8902, Train Acc: 0.6838, Test Loss: 1.0131, Test Acc: 0.6441\n","Epoch [39/200], Train Loss: 0.8766, Train Acc: 0.6901, Test Loss: 0.9104, Test Acc: 0.6768\n","Epoch [40/200], Train Loss: 0.8572, Train Acc: 0.6952, Test Loss: 0.9817, Test Acc: 0.6530\n","Epoch [41/200], Train Loss: 0.8558, Train Acc: 0.6974, Test Loss: 0.8627, Test Acc: 0.6968\n","Epoch [42/200], Train Loss: 0.8337, Train Acc: 0.7054, Test Loss: 0.9875, Test Acc: 0.6589\n","Epoch [43/200], Train Loss: 0.8316, Train Acc: 0.7063, Test Loss: 0.8261, Test Acc: 0.7053\n","Epoch [44/200], Train Loss: 0.8129, Train Acc: 0.7115, Test Loss: 0.8557, Test Acc: 0.7004\n","Epoch [45/200], Train Loss: 0.8060, Train Acc: 0.7158, Test Loss: 0.9477, Test Acc: 0.6760\n","Epoch [46/200], Train Loss: 0.7905, Train Acc: 0.7209, Test Loss: 1.1041, Test Acc: 0.6253\n","Epoch [47/200], Train Loss: 0.7872, Train Acc: 0.7223, Test Loss: 1.0215, Test Acc: 0.6458\n","Epoch [48/200], Train Loss: 0.7709, Train Acc: 0.7273, Test Loss: 0.9340, Test Acc: 0.6822\n","Epoch [49/200], Train Loss: 0.7669, Train Acc: 0.7312, Test Loss: 0.9669, Test Acc: 0.6770\n","Epoch [50/200], Train Loss: 0.7514, Train Acc: 0.7343, Test Loss: 0.8292, Test Acc: 0.7088\n","Epoch [51/200], Train Loss: 0.7428, Train Acc: 0.7390, Test Loss: 0.8099, Test Acc: 0.7199\n","Epoch [52/200], Train Loss: 0.7312, Train Acc: 0.7436, Test Loss: 0.8165, Test Acc: 0.7160\n","Epoch [53/200], Train Loss: 0.7182, Train Acc: 0.7482, Test Loss: 0.7734, Test Acc: 0.7317\n","Epoch [54/200], Train Loss: 0.7113, Train Acc: 0.7502, Test Loss: 0.8916, Test Acc: 0.6833\n","Epoch [55/200], Train Loss: 0.6949, Train Acc: 0.7562, Test Loss: 0.8726, Test Acc: 0.7065\n","Epoch [56/200], Train Loss: 0.6934, Train Acc: 0.7592, Test Loss: 0.9106, Test Acc: 0.6821\n","Epoch [57/200], Train Loss: 0.6785, Train Acc: 0.7632, Test Loss: 1.1439, Test Acc: 0.6155\n","Epoch [58/200], Train Loss: 0.6708, Train Acc: 0.7645, Test Loss: 0.8252, Test Acc: 0.7229\n","Epoch [59/200], Train Loss: 0.6650, Train Acc: 0.7677, Test Loss: 0.8354, Test Acc: 0.7122\n","Epoch [60/200], Train Loss: 0.6660, Train Acc: 0.7678, Test Loss: 0.7355, Test Acc: 0.7427\n","Epoch [61/200], Train Loss: 0.6528, Train Acc: 0.7738, Test Loss: 0.8296, Test Acc: 0.7162\n","Epoch [62/200], Train Loss: 0.6482, Train Acc: 0.7728, Test Loss: 0.7826, Test Acc: 0.7347\n","Epoch [63/200], Train Loss: 0.6414, Train Acc: 0.7764, Test Loss: 0.7463, Test Acc: 0.7422\n","Epoch [64/200], Train Loss: 0.6457, Train Acc: 0.7748, Test Loss: 0.7700, Test Acc: 0.7371\n","Epoch [65/200], Train Loss: 0.6338, Train Acc: 0.7803, Test Loss: 0.8721, Test Acc: 0.7087\n","Epoch [66/200], Train Loss: 0.6304, Train Acc: 0.7823, Test Loss: 0.6894, Test Acc: 0.7644\n","Epoch [67/200], Train Loss: 0.6183, Train Acc: 0.7851, Test Loss: 0.8719, Test Acc: 0.7090\n","Epoch [68/200], Train Loss: 0.6218, Train Acc: 0.7831, Test Loss: 0.8117, Test Acc: 0.7200\n","Epoch [69/200], Train Loss: 0.6213, Train Acc: 0.7842, Test Loss: 0.6991, Test Acc: 0.7558\n","Epoch [70/200], Train Loss: 0.6123, Train Acc: 0.7884, Test Loss: 0.9033, Test Acc: 0.7022\n","Epoch [71/200], Train Loss: 0.6124, Train Acc: 0.7868, Test Loss: 1.0592, Test Acc: 0.6558\n","Epoch [72/200], Train Loss: 0.6166, Train Acc: 0.7851, Test Loss: 0.7266, Test Acc: 0.7530\n","Epoch [73/200], Train Loss: 0.6043, Train Acc: 0.7923, Test Loss: 0.7723, Test Acc: 0.7344\n","Epoch [74/200], Train Loss: 0.6036, Train Acc: 0.7894, Test Loss: 0.6567, Test Acc: 0.7768\n","Epoch [75/200], Train Loss: 0.6007, Train Acc: 0.7914, Test Loss: 0.6794, Test Acc: 0.7657\n","Epoch [76/200], Train Loss: 0.5945, Train Acc: 0.7924, Test Loss: 0.6370, Test Acc: 0.7785\n","Epoch [77/200], Train Loss: 0.5841, Train Acc: 0.7973, Test Loss: 0.7132, Test Acc: 0.7575\n","Epoch [78/200], Train Loss: 0.5910, Train Acc: 0.7943, Test Loss: 0.7132, Test Acc: 0.7571\n","Epoch [79/200], Train Loss: 0.5875, Train Acc: 0.7952, Test Loss: 0.6508, Test Acc: 0.7811\n","Epoch [80/200], Train Loss: 0.5909, Train Acc: 0.7949, Test Loss: 0.7473, Test Acc: 0.7445\n","Epoch [81/200], Train Loss: 0.5841, Train Acc: 0.7977, Test Loss: 0.7312, Test Acc: 0.7544\n","Epoch [82/200], Train Loss: 0.5858, Train Acc: 0.7961, Test Loss: 0.7664, Test Acc: 0.7419\n","Epoch [83/200], Train Loss: 0.5835, Train Acc: 0.7953, Test Loss: 0.8264, Test Acc: 0.7224\n","Epoch [84/200], Train Loss: 0.5799, Train Acc: 0.7981, Test Loss: 0.7112, Test Acc: 0.7625\n","Epoch [85/200], Train Loss: 0.5746, Train Acc: 0.7990, Test Loss: 0.6969, Test Acc: 0.7577\n","Epoch [86/200], Train Loss: 0.5705, Train Acc: 0.8009, Test Loss: 0.8088, Test Acc: 0.7300\n","Epoch [87/200], Train Loss: 0.5660, Train Acc: 0.8036, Test Loss: 0.6668, Test Acc: 0.7704\n","Epoch [88/200], Train Loss: 0.5612, Train Acc: 0.8056, Test Loss: 0.6908, Test Acc: 0.7561\n","Epoch [89/200], Train Loss: 0.5623, Train Acc: 0.8045, Test Loss: 0.7828, Test Acc: 0.7437\n","Epoch [90/200], Train Loss: 0.5630, Train Acc: 0.8049, Test Loss: 0.7365, Test Acc: 0.7433\n","Epoch [91/200], Train Loss: 0.5642, Train Acc: 0.8048, Test Loss: 0.7754, Test Acc: 0.7400\n","Epoch [92/200], Train Loss: 0.5686, Train Acc: 0.8030, Test Loss: 0.6897, Test Acc: 0.7695\n","Epoch [93/200], Train Loss: 0.5580, Train Acc: 0.8057, Test Loss: 0.6596, Test Acc: 0.7729\n","Epoch [94/200], Train Loss: 0.5555, Train Acc: 0.8061, Test Loss: 0.8220, Test Acc: 0.7304\n","Epoch [95/200], Train Loss: 0.5612, Train Acc: 0.8057, Test Loss: 0.8152, Test Acc: 0.7223\n","Epoch [96/200], Train Loss: 0.5516, Train Acc: 0.8079, Test Loss: 0.7110, Test Acc: 0.7640\n","Epoch [97/200], Train Loss: 0.5522, Train Acc: 0.8082, Test Loss: 0.8391, Test Acc: 0.7253\n","Epoch [98/200], Train Loss: 0.5495, Train Acc: 0.8090, Test Loss: 0.8274, Test Acc: 0.7200\n","Epoch [99/200], Train Loss: 0.5511, Train Acc: 0.8094, Test Loss: 0.6752, Test Acc: 0.7720\n","Epoch [100/200], Train Loss: 0.5497, Train Acc: 0.8077, Test Loss: 0.6543, Test Acc: 0.7789\n","Epoch [101/200], Train Loss: 0.5469, Train Acc: 0.8089, Test Loss: 0.6924, Test Acc: 0.7652\n","Epoch [102/200], Train Loss: 0.5443, Train Acc: 0.8098, Test Loss: 0.6875, Test Acc: 0.7743\n","Epoch [103/200], Train Loss: 0.5457, Train Acc: 0.8108, Test Loss: 0.7434, Test Acc: 0.7601\n","Epoch [104/200], Train Loss: 0.5461, Train Acc: 0.8108, Test Loss: 0.6797, Test Acc: 0.7680\n","Epoch [105/200], Train Loss: 0.5422, Train Acc: 0.8114, Test Loss: 0.6626, Test Acc: 0.7692\n","Epoch [106/200], Train Loss: 0.5396, Train Acc: 0.8104, Test Loss: 0.6295, Test Acc: 0.7821\n","Epoch [107/200], Train Loss: 0.5445, Train Acc: 0.8113, Test Loss: 0.7583, Test Acc: 0.7536\n","Epoch [108/200], Train Loss: 0.5351, Train Acc: 0.8150, Test Loss: 0.6767, Test Acc: 0.7683\n","Epoch [109/200], Train Loss: 0.5360, Train Acc: 0.8145, Test Loss: 0.8557, Test Acc: 0.7148\n","Epoch [110/200], Train Loss: 0.5369, Train Acc: 0.8133, Test Loss: 0.7531, Test Acc: 0.7394\n","Epoch [111/200], Train Loss: 0.5372, Train Acc: 0.8139, Test Loss: 0.6306, Test Acc: 0.7807\n","Epoch [112/200], Train Loss: 0.5325, Train Acc: 0.8160, Test Loss: 0.6925, Test Acc: 0.7640\n","Epoch [113/200], Train Loss: 0.5320, Train Acc: 0.8149, Test Loss: 0.7295, Test Acc: 0.7583\n","Epoch [114/200], Train Loss: 0.5309, Train Acc: 0.8137, Test Loss: 0.6101, Test Acc: 0.7926\n","Epoch [115/200], Train Loss: 0.5317, Train Acc: 0.8145, Test Loss: 0.7216, Test Acc: 0.7605\n","Epoch [116/200], Train Loss: 0.5304, Train Acc: 0.8149, Test Loss: 1.0306, Test Acc: 0.6892\n","Epoch [117/200], Train Loss: 0.5250, Train Acc: 0.8176, Test Loss: 0.6425, Test Acc: 0.7871\n","Epoch [118/200], Train Loss: 0.5268, Train Acc: 0.8163, Test Loss: 0.7129, Test Acc: 0.7595\n","Epoch [119/200], Train Loss: 0.5266, Train Acc: 0.8166, Test Loss: 0.6170, Test Acc: 0.7897\n","Epoch [120/200], Train Loss: 0.5229, Train Acc: 0.8191, Test Loss: 0.6878, Test Acc: 0.7724\n","Epoch [121/200], Train Loss: 0.5186, Train Acc: 0.8192, Test Loss: 0.6239, Test Acc: 0.7871\n","Epoch [122/200], Train Loss: 0.5227, Train Acc: 0.8185, Test Loss: 0.8365, Test Acc: 0.7232\n","Epoch [123/200], Train Loss: 0.5164, Train Acc: 0.8216, Test Loss: 0.9465, Test Acc: 0.7117\n","Epoch [124/200], Train Loss: 0.5195, Train Acc: 0.8197, Test Loss: 0.7316, Test Acc: 0.7589\n","Epoch [125/200], Train Loss: 0.5193, Train Acc: 0.8192, Test Loss: 0.7099, Test Acc: 0.7651\n","Epoch [126/200], Train Loss: 0.5217, Train Acc: 0.8179, Test Loss: 0.8395, Test Acc: 0.7266\n","Epoch [127/200], Train Loss: 0.5204, Train Acc: 0.8195, Test Loss: 0.7346, Test Acc: 0.7508\n","Epoch [128/200], Train Loss: 0.5194, Train Acc: 0.8185, Test Loss: 0.6644, Test Acc: 0.7819\n","Epoch [129/200], Train Loss: 0.5177, Train Acc: 0.8190, Test Loss: 0.6361, Test Acc: 0.7868\n","Epoch [130/200], Train Loss: 0.5181, Train Acc: 0.8198, Test Loss: 0.7365, Test Acc: 0.7640\n","Epoch [131/200], Train Loss: 0.5172, Train Acc: 0.8211, Test Loss: 0.6679, Test Acc: 0.7706\n","Epoch [132/200], Train Loss: 0.5113, Train Acc: 0.8228, Test Loss: 0.7975, Test Acc: 0.7452\n","Epoch [133/200], Train Loss: 0.5124, Train Acc: 0.8212, Test Loss: 0.6018, Test Acc: 0.8015\n","Epoch [134/200], Train Loss: 0.5197, Train Acc: 0.8198, Test Loss: 0.6940, Test Acc: 0.7631\n","Epoch [135/200], Train Loss: 0.5166, Train Acc: 0.8215, Test Loss: 0.6235, Test Acc: 0.7907\n","Epoch [136/200], Train Loss: 0.5084, Train Acc: 0.8237, Test Loss: 0.6755, Test Acc: 0.7714\n","Epoch [137/200], Train Loss: 0.5120, Train Acc: 0.8206, Test Loss: 0.7415, Test Acc: 0.7520\n","Epoch [138/200], Train Loss: 0.5085, Train Acc: 0.8243, Test Loss: 0.6891, Test Acc: 0.7732\n","Epoch [139/200], Train Loss: 0.5123, Train Acc: 0.8208, Test Loss: 0.8337, Test Acc: 0.7225\n","Epoch [140/200], Train Loss: 0.5023, Train Acc: 0.8261, Test Loss: 0.8094, Test Acc: 0.7349\n","Epoch [141/200], Train Loss: 0.5070, Train Acc: 0.8249, Test Loss: 0.6884, Test Acc: 0.7727\n","Epoch [142/200], Train Loss: 0.5064, Train Acc: 0.8252, Test Loss: 0.7043, Test Acc: 0.7689\n","Epoch [143/200], Train Loss: 0.5100, Train Acc: 0.8229, Test Loss: 0.6394, Test Acc: 0.7825\n","Epoch [144/200], Train Loss: 0.5120, Train Acc: 0.8221, Test Loss: 0.6736, Test Acc: 0.7716\n","Epoch [145/200], Train Loss: 0.5128, Train Acc: 0.8213, Test Loss: 0.6357, Test Acc: 0.7862\n","Epoch [146/200], Train Loss: 0.5063, Train Acc: 0.8240, Test Loss: 0.7631, Test Acc: 0.7483\n","Epoch [147/200], Train Loss: 0.5024, Train Acc: 0.8262, Test Loss: 0.7443, Test Acc: 0.7550\n","Epoch [148/200], Train Loss: 0.5039, Train Acc: 0.8238, Test Loss: 0.5770, Test Acc: 0.8060\n","Epoch [149/200], Train Loss: 0.5100, Train Acc: 0.8222, Test Loss: 0.6677, Test Acc: 0.7707\n","Epoch [150/200], Train Loss: 0.5078, Train Acc: 0.8224, Test Loss: 0.6751, Test Acc: 0.7746\n","Epoch [151/200], Train Loss: 0.5002, Train Acc: 0.8264, Test Loss: 0.7610, Test Acc: 0.7500\n","Epoch [152/200], Train Loss: 0.4996, Train Acc: 0.8248, Test Loss: 0.6811, Test Acc: 0.7742\n","Epoch [153/200], Train Loss: 0.5004, Train Acc: 0.8262, Test Loss: 0.7525, Test Acc: 0.7489\n","Epoch [154/200], Train Loss: 0.5004, Train Acc: 0.8263, Test Loss: 0.7197, Test Acc: 0.7655\n","Epoch [155/200], Train Loss: 0.4936, Train Acc: 0.8287, Test Loss: 0.7132, Test Acc: 0.7728\n","Epoch [156/200], Train Loss: 0.5051, Train Acc: 0.8240, Test Loss: 0.6008, Test Acc: 0.8000\n","Epoch [157/200], Train Loss: 0.4965, Train Acc: 0.8253, Test Loss: 0.6147, Test Acc: 0.7886\n","Epoch [158/200], Train Loss: 0.4986, Train Acc: 0.8273, Test Loss: 0.8090, Test Acc: 0.7337\n","Epoch [159/200], Train Loss: 0.4972, Train Acc: 0.8277, Test Loss: 0.6153, Test Acc: 0.7931\n","Epoch [160/200], Train Loss: 0.4968, Train Acc: 0.8278, Test Loss: 0.6636, Test Acc: 0.7764\n","Epoch [161/200], Train Loss: 0.5000, Train Acc: 0.8276, Test Loss: 0.6626, Test Acc: 0.7757\n","Epoch [162/200], Train Loss: 0.4929, Train Acc: 0.8294, Test Loss: 0.6673, Test Acc: 0.7766\n","Epoch [163/200], Train Loss: 0.4935, Train Acc: 0.8282, Test Loss: 0.6885, Test Acc: 0.7735\n","Epoch [164/200], Train Loss: 0.4918, Train Acc: 0.8293, Test Loss: 0.6669, Test Acc: 0.7712\n","Epoch [165/200], Train Loss: 0.4879, Train Acc: 0.8320, Test Loss: 0.7273, Test Acc: 0.7578\n","Epoch [166/200], Train Loss: 0.4980, Train Acc: 0.8283, Test Loss: 0.6186, Test Acc: 0.7927\n","Epoch [167/200], Train Loss: 0.4906, Train Acc: 0.8283, Test Loss: 0.7393, Test Acc: 0.7612\n","Epoch [168/200], Train Loss: 0.4932, Train Acc: 0.8272, Test Loss: 0.6497, Test Acc: 0.7841\n","Epoch [169/200], Train Loss: 0.4940, Train Acc: 0.8260, Test Loss: 0.6553, Test Acc: 0.7765\n","Epoch [170/200], Train Loss: 0.4911, Train Acc: 0.8303, Test Loss: 0.6324, Test Acc: 0.7835\n","Epoch [171/200], Train Loss: 0.4862, Train Acc: 0.8330, Test Loss: 0.8395, Test Acc: 0.7245\n","Epoch [172/200], Train Loss: 0.4896, Train Acc: 0.8298, Test Loss: 0.7242, Test Acc: 0.7575\n","Epoch [173/200], Train Loss: 0.4911, Train Acc: 0.8287, Test Loss: 0.7413, Test Acc: 0.7583\n","Epoch [174/200], Train Loss: 0.4860, Train Acc: 0.8319, Test Loss: 0.6202, Test Acc: 0.7870\n","Epoch [175/200], Train Loss: 0.4865, Train Acc: 0.8297, Test Loss: 0.6352, Test Acc: 0.7857\n","Epoch [176/200], Train Loss: 0.4981, Train Acc: 0.8262, Test Loss: 0.6975, Test Acc: 0.7697\n","Epoch [177/200], Train Loss: 0.4824, Train Acc: 0.8304, Test Loss: 0.7447, Test Acc: 0.7521\n","Epoch [178/200], Train Loss: 0.4843, Train Acc: 0.8310, Test Loss: 0.6458, Test Acc: 0.7792\n","Epoch [179/200], Train Loss: 0.4864, Train Acc: 0.8314, Test Loss: 0.7906, Test Acc: 0.7334\n","Epoch [180/200], Train Loss: 0.4880, Train Acc: 0.8296, Test Loss: 0.6831, Test Acc: 0.7761\n","Epoch [181/200], Train Loss: 0.4834, Train Acc: 0.8325, Test Loss: 0.6483, Test Acc: 0.7808\n","Epoch [182/200], Train Loss: 0.4835, Train Acc: 0.8319, Test Loss: 0.6438, Test Acc: 0.7807\n","Epoch [183/200], Train Loss: 0.4893, Train Acc: 0.8280, Test Loss: 0.7729, Test Acc: 0.7443\n","Epoch [184/200], Train Loss: 0.4811, Train Acc: 0.8324, Test Loss: 0.6323, Test Acc: 0.7897\n","Epoch [185/200], Train Loss: 0.4864, Train Acc: 0.8308, Test Loss: 0.6646, Test Acc: 0.7770\n","Epoch [186/200], Train Loss: 0.4775, Train Acc: 0.8330, Test Loss: 0.6130, Test Acc: 0.7889\n","Epoch [187/200], Train Loss: 0.4830, Train Acc: 0.8316, Test Loss: 0.6078, Test Acc: 0.7994\n","Epoch [188/200], Train Loss: 0.4843, Train Acc: 0.8320, Test Loss: 0.7533, Test Acc: 0.7504\n","Epoch [189/200], Train Loss: 0.4784, Train Acc: 0.8317, Test Loss: 0.6360, Test Acc: 0.7867\n","Epoch [190/200], Train Loss: 0.4833, Train Acc: 0.8304, Test Loss: 0.6030, Test Acc: 0.7912\n","Epoch [191/200], Train Loss: 0.4787, Train Acc: 0.8346, Test Loss: 0.6195, Test Acc: 0.7938\n","Epoch [192/200], Train Loss: 0.4835, Train Acc: 0.8319, Test Loss: 0.7248, Test Acc: 0.7652\n","Epoch [193/200], Train Loss: 0.4870, Train Acc: 0.8291, Test Loss: 0.6000, Test Acc: 0.8002\n","Epoch [194/200], Train Loss: 0.4777, Train Acc: 0.8318, Test Loss: 0.6418, Test Acc: 0.7857\n","Epoch [195/200], Train Loss: 0.4777, Train Acc: 0.8338, Test Loss: 0.6032, Test Acc: 0.7918\n","Epoch [196/200], Train Loss: 0.4836, Train Acc: 0.8313, Test Loss: 0.7632, Test Acc: 0.7568\n","Epoch [197/200], Train Loss: 0.4768, Train Acc: 0.8343, Test Loss: 0.6037, Test Acc: 0.7967\n","Epoch [198/200], Train Loss: 0.4837, Train Acc: 0.8311, Test Loss: 0.6239, Test Acc: 0.7850\n","Epoch [199/200], Train Loss: 0.4767, Train Acc: 0.8328, Test Loss: 0.7735, Test Acc: 0.7454\n","Epoch [200/200], Train Loss: 0.4817, Train Acc: 0.8321, Test Loss: 0.6024, Test Acc: 0.7950\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","\n","# Set device to use\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define data transformations\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","# Load CIFAR10 dataset\n","trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","\n","# Define data loaders\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","# Define MobileNetV3 model\n","model = models.mobilenet_v3_large(pretrained=False, num_classes=10).to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n","\n","# Train the model\n","num_epochs = 200\n","for epoch in range(num_epochs):\n","    # Train the model\n","    model.train()\n","    train_loss = 0\n","    train_correct = 0\n","    for images, labels in trainloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item() * images.size(0)\n","        train_correct += (outputs.argmax(dim=1) == labels).sum().item()\n","\n","    # Test the model\n","    model.eval()\n","    test_loss = 0\n","    test_correct = 0\n","    with torch.no_grad():\n","        for images, labels in testloader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            test_loss += loss.item() * images.size(0)\n","            test_correct += (outputs.argmax(dim=1) == labels).sum().item()\n","\n","    # Print results for this epoch\n","    train_loss /= len(trainset)\n","    train_acc = train_correct / len(trainset)\n","    test_loss /= len(testset)\n","    test_acc = test_correct / len(testset)\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n","\n","# Save the trained model\n","torch.save(model.state_dict(), 'mobilenetv3_cifar10.pth')\n"]},{"cell_type":"markdown","metadata":{},"source":["## VGG-16"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-04-10T01:59:14.478444Z","iopub.status.busy":"2023-04-10T01:59:14.477554Z","iopub.status.idle":"2023-04-10T01:59:16.984442Z","shell.execute_reply":"2023-04-10T01:59:16.983414Z","shell.execute_reply.started":"2023-04-10T01:59:14.478404Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","\n","# Set device to use\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# Define VGG16 model\n","class VGG16(nn.Module):\n","    def __init__(self, num_classes=1000):\n","        super(VGG16, self).__init__()\n","\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(4096, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-10T01:59:16.986827Z","iopub.status.busy":"2023-04-10T01:59:16.986354Z","iopub.status.idle":"2023-04-10T03:48:56.565225Z","shell.execute_reply":"2023-04-10T03:48:56.563735Z","shell.execute_reply.started":"2023-04-10T01:59:16.986798Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76b273e1123b47908cd4777b41ef20b3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n","/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/200], Train Loss: 2.0651, Train Acc: 0.1980, Test Loss: 1.8895, Test Acc: 0.2341\n","Epoch [2/200], Train Loss: 1.6476, Train Acc: 0.3668, Test Loss: 1.3698, Test Acc: 0.4872\n","Epoch [3/200], Train Loss: 1.3697, Train Acc: 0.4968, Test Loss: 1.1837, Test Acc: 0.5623\n","Epoch [4/200], Train Loss: 1.1515, Train Acc: 0.5835, Test Loss: 0.9715, Test Acc: 0.6455\n","Epoch [5/200], Train Loss: 1.0042, Train Acc: 0.6444, Test Loss: 0.9004, Test Acc: 0.6852\n","Epoch [6/200], Train Loss: 0.8863, Train Acc: 0.6874, Test Loss: 0.9415, Test Acc: 0.6665\n","Epoch [7/200], Train Loss: 0.8087, Train Acc: 0.7169, Test Loss: 0.7593, Test Acc: 0.7406\n","Epoch [8/200], Train Loss: 0.7298, Train Acc: 0.7464, Test Loss: 0.6944, Test Acc: 0.7621\n","Epoch [9/200], Train Loss: 0.6648, Train Acc: 0.7723, Test Loss: 0.6350, Test Acc: 0.7801\n","Epoch [10/200], Train Loss: 0.6169, Train Acc: 0.7905, Test Loss: 0.5915, Test Acc: 0.8048\n","Epoch [11/200], Train Loss: 0.5759, Train Acc: 0.8044, Test Loss: 0.6763, Test Acc: 0.7765\n","Epoch [12/200], Train Loss: 0.5375, Train Acc: 0.8182, Test Loss: 0.5376, Test Acc: 0.8191\n","Epoch [13/200], Train Loss: 0.5132, Train Acc: 0.8240, Test Loss: 0.5225, Test Acc: 0.8300\n","Epoch [14/200], Train Loss: 0.4831, Train Acc: 0.8365, Test Loss: 0.5783, Test Acc: 0.8094\n","Epoch [15/200], Train Loss: 0.4547, Train Acc: 0.8464, Test Loss: 0.4693, Test Acc: 0.8407\n","Epoch [16/200], Train Loss: 0.4335, Train Acc: 0.8536, Test Loss: 0.5448, Test Acc: 0.8222\n","Epoch [17/200], Train Loss: 0.4118, Train Acc: 0.8620, Test Loss: 0.4916, Test Acc: 0.8386\n","Epoch [18/200], Train Loss: 0.3907, Train Acc: 0.8684, Test Loss: 0.4543, Test Acc: 0.8518\n","Epoch [19/200], Train Loss: 0.3666, Train Acc: 0.8766, Test Loss: 0.4458, Test Acc: 0.8563\n","Epoch [20/200], Train Loss: 0.3505, Train Acc: 0.8818, Test Loss: 0.4573, Test Acc: 0.8516\n","Epoch [21/200], Train Loss: 0.3403, Train Acc: 0.8851, Test Loss: 0.4617, Test Acc: 0.8431\n","Epoch [22/200], Train Loss: 0.3288, Train Acc: 0.8886, Test Loss: 0.4388, Test Acc: 0.8562\n","Epoch [23/200], Train Loss: 0.3160, Train Acc: 0.8922, Test Loss: 0.4502, Test Acc: 0.8564\n","Epoch [24/200], Train Loss: 0.3036, Train Acc: 0.8977, Test Loss: 0.4264, Test Acc: 0.8648\n","Epoch [25/200], Train Loss: 0.2927, Train Acc: 0.9016, Test Loss: 0.4273, Test Acc: 0.8595\n","Epoch [26/200], Train Loss: 0.2792, Train Acc: 0.9054, Test Loss: 0.4338, Test Acc: 0.8592\n","Epoch [27/200], Train Loss: 0.2672, Train Acc: 0.9101, Test Loss: 0.4731, Test Acc: 0.8569\n","Epoch [28/200], Train Loss: 0.2559, Train Acc: 0.9129, Test Loss: 0.4036, Test Acc: 0.8670\n","Epoch [29/200], Train Loss: 0.2431, Train Acc: 0.9175, Test Loss: 0.4081, Test Acc: 0.8749\n","Epoch [30/200], Train Loss: 0.2349, Train Acc: 0.9223, Test Loss: 0.4572, Test Acc: 0.8634\n","Epoch [31/200], Train Loss: 0.2286, Train Acc: 0.9218, Test Loss: 0.3895, Test Acc: 0.8778\n","Epoch [32/200], Train Loss: 0.2167, Train Acc: 0.9270, Test Loss: 0.4456, Test Acc: 0.8632\n","Epoch [33/200], Train Loss: 0.2266, Train Acc: 0.9220, Test Loss: 0.4253, Test Acc: 0.8708\n","Epoch [34/200], Train Loss: 0.2085, Train Acc: 0.9298, Test Loss: 0.4536, Test Acc: 0.8634\n","Epoch [35/200], Train Loss: 0.1989, Train Acc: 0.9332, Test Loss: 0.4269, Test Acc: 0.8685\n","Epoch [36/200], Train Loss: 0.1957, Train Acc: 0.9338, Test Loss: 0.4276, Test Acc: 0.8715\n","Epoch [37/200], Train Loss: 0.1913, Train Acc: 0.9351, Test Loss: 0.4103, Test Acc: 0.8774\n","Epoch [38/200], Train Loss: 0.1841, Train Acc: 0.9382, Test Loss: 0.4226, Test Acc: 0.8730\n","Epoch [39/200], Train Loss: 0.1802, Train Acc: 0.9398, Test Loss: 0.4290, Test Acc: 0.8728\n","Epoch [40/200], Train Loss: 0.1748, Train Acc: 0.9408, Test Loss: 0.4425, Test Acc: 0.8687\n","Epoch [41/200], Train Loss: 0.1712, Train Acc: 0.9418, Test Loss: 0.3926, Test Acc: 0.8769\n","Epoch [42/200], Train Loss: 0.1612, Train Acc: 0.9459, Test Loss: 0.3777, Test Acc: 0.8808\n","Epoch [43/200], Train Loss: 0.1701, Train Acc: 0.9430, Test Loss: 0.4332, Test Acc: 0.8804\n","Epoch [44/200], Train Loss: 0.1616, Train Acc: 0.9449, Test Loss: 0.4038, Test Acc: 0.8796\n","Epoch [45/200], Train Loss: 0.1602, Train Acc: 0.9469, Test Loss: 0.3876, Test Acc: 0.8847\n","Epoch [46/200], Train Loss: 0.1501, Train Acc: 0.9498, Test Loss: 0.4369, Test Acc: 0.8723\n","Epoch [47/200], Train Loss: 0.1451, Train Acc: 0.9503, Test Loss: 0.4192, Test Acc: 0.8785\n","Epoch [48/200], Train Loss: 0.1517, Train Acc: 0.9481, Test Loss: 0.3926, Test Acc: 0.8873\n","Epoch [49/200], Train Loss: 0.1377, Train Acc: 0.9540, Test Loss: 0.4196, Test Acc: 0.8810\n","Epoch [50/200], Train Loss: 0.1440, Train Acc: 0.9509, Test Loss: 0.4013, Test Acc: 0.8861\n","Epoch [51/200], Train Loss: 0.1324, Train Acc: 0.9549, Test Loss: 0.4234, Test Acc: 0.8769\n","Epoch [52/200], Train Loss: 0.1283, Train Acc: 0.9558, Test Loss: 0.4244, Test Acc: 0.8808\n","Epoch [53/200], Train Loss: 0.1316, Train Acc: 0.9552, Test Loss: 0.4181, Test Acc: 0.8829\n","Epoch [54/200], Train Loss: 0.1337, Train Acc: 0.9549, Test Loss: 0.4328, Test Acc: 0.8746\n","Epoch [55/200], Train Loss: 0.1242, Train Acc: 0.9579, Test Loss: 0.4247, Test Acc: 0.8853\n","Epoch [56/200], Train Loss: 0.1328, Train Acc: 0.9554, Test Loss: 0.3971, Test Acc: 0.8827\n","Epoch [57/200], Train Loss: 0.1221, Train Acc: 0.9584, Test Loss: 0.4254, Test Acc: 0.8820\n","Epoch [58/200], Train Loss: 0.1270, Train Acc: 0.9573, Test Loss: 0.4255, Test Acc: 0.8851\n","Epoch [59/200], Train Loss: 0.1152, Train Acc: 0.9606, Test Loss: 0.4015, Test Acc: 0.8875\n","Epoch [60/200], Train Loss: 0.1173, Train Acc: 0.9605, Test Loss: 0.4166, Test Acc: 0.8838\n","Epoch [61/200], Train Loss: 0.1150, Train Acc: 0.9607, Test Loss: 0.4615, Test Acc: 0.8757\n","Epoch [62/200], Train Loss: 0.1127, Train Acc: 0.9627, Test Loss: 0.3662, Test Acc: 0.8939\n","Epoch [63/200], Train Loss: 0.1078, Train Acc: 0.9640, Test Loss: 0.4175, Test Acc: 0.8853\n","Epoch [64/200], Train Loss: 0.1089, Train Acc: 0.9627, Test Loss: 0.4111, Test Acc: 0.8888\n","Epoch [65/200], Train Loss: 0.1071, Train Acc: 0.9646, Test Loss: 0.3931, Test Acc: 0.8853\n","Epoch [66/200], Train Loss: 0.1032, Train Acc: 0.9644, Test Loss: 0.4283, Test Acc: 0.8855\n","Epoch [67/200], Train Loss: 0.1058, Train Acc: 0.9639, Test Loss: 0.4237, Test Acc: 0.8843\n","Epoch [68/200], Train Loss: 0.1021, Train Acc: 0.9656, Test Loss: 0.4119, Test Acc: 0.8842\n","Epoch [69/200], Train Loss: 0.1042, Train Acc: 0.9641, Test Loss: 0.4816, Test Acc: 0.8736\n","Epoch [70/200], Train Loss: 0.1021, Train Acc: 0.9654, Test Loss: 0.4371, Test Acc: 0.8818\n","Epoch [71/200], Train Loss: 0.0955, Train Acc: 0.9668, Test Loss: 0.4229, Test Acc: 0.8897\n","Epoch [72/200], Train Loss: 0.1025, Train Acc: 0.9649, Test Loss: 0.3809, Test Acc: 0.8921\n","Epoch [73/200], Train Loss: 0.0960, Train Acc: 0.9681, Test Loss: 0.4604, Test Acc: 0.8785\n","Epoch [74/200], Train Loss: 0.0964, Train Acc: 0.9666, Test Loss: 0.4368, Test Acc: 0.8840\n","Epoch [75/200], Train Loss: 0.0960, Train Acc: 0.9676, Test Loss: 0.4464, Test Acc: 0.8788\n","Epoch [76/200], Train Loss: 0.0961, Train Acc: 0.9677, Test Loss: 0.4168, Test Acc: 0.8814\n","Epoch [77/200], Train Loss: 0.0960, Train Acc: 0.9675, Test Loss: 0.3930, Test Acc: 0.8938\n","Epoch [78/200], Train Loss: 0.0895, Train Acc: 0.9696, Test Loss: 0.4219, Test Acc: 0.8869\n","Epoch [79/200], Train Loss: 0.0886, Train Acc: 0.9697, Test Loss: 0.3927, Test Acc: 0.8915\n","Epoch [80/200], Train Loss: 0.0960, Train Acc: 0.9673, Test Loss: 0.4140, Test Acc: 0.8878\n","Epoch [81/200], Train Loss: 0.0999, Train Acc: 0.9664, Test Loss: 0.4381, Test Acc: 0.8848\n","Epoch [82/200], Train Loss: 0.0918, Train Acc: 0.9695, Test Loss: 0.4482, Test Acc: 0.8832\n","Epoch [83/200], Train Loss: 0.0868, Train Acc: 0.9701, Test Loss: 0.3969, Test Acc: 0.8947\n","Epoch [84/200], Train Loss: 0.0900, Train Acc: 0.9696, Test Loss: 0.4094, Test Acc: 0.8917\n","Epoch [85/200], Train Loss: 0.0878, Train Acc: 0.9698, Test Loss: 0.4161, Test Acc: 0.8854\n","Epoch [86/200], Train Loss: 0.0904, Train Acc: 0.9690, Test Loss: 0.3721, Test Acc: 0.8930\n","Epoch [87/200], Train Loss: 0.0807, Train Acc: 0.9729, Test Loss: 0.4409, Test Acc: 0.8868\n","Epoch [88/200], Train Loss: 0.0893, Train Acc: 0.9701, Test Loss: 0.3906, Test Acc: 0.8945\n","Epoch [89/200], Train Loss: 0.0839, Train Acc: 0.9723, Test Loss: 0.4327, Test Acc: 0.8871\n","Epoch [90/200], Train Loss: 0.0837, Train Acc: 0.9721, Test Loss: 0.4421, Test Acc: 0.8851\n","Epoch [91/200], Train Loss: 0.0885, Train Acc: 0.9702, Test Loss: 0.4439, Test Acc: 0.8890\n","Epoch [92/200], Train Loss: 0.0853, Train Acc: 0.9708, Test Loss: 0.4013, Test Acc: 0.8906\n","Epoch [93/200], Train Loss: 0.0800, Train Acc: 0.9727, Test Loss: 0.4266, Test Acc: 0.8873\n","Epoch [94/200], Train Loss: 0.0794, Train Acc: 0.9728, Test Loss: 0.4202, Test Acc: 0.8881\n","Epoch [95/200], Train Loss: 0.0796, Train Acc: 0.9727, Test Loss: 0.4268, Test Acc: 0.8852\n","Epoch [96/200], Train Loss: 0.0834, Train Acc: 0.9713, Test Loss: 0.3804, Test Acc: 0.8926\n","Epoch [97/200], Train Loss: 0.0797, Train Acc: 0.9734, Test Loss: 0.4262, Test Acc: 0.8861\n","Epoch [98/200], Train Loss: 0.0849, Train Acc: 0.9718, Test Loss: 0.4112, Test Acc: 0.8892\n","Epoch [99/200], Train Loss: 0.0814, Train Acc: 0.9725, Test Loss: 0.4628, Test Acc: 0.8843\n","Epoch [100/200], Train Loss: 0.0789, Train Acc: 0.9729, Test Loss: 0.4492, Test Acc: 0.8876\n","Epoch [101/200], Train Loss: 0.0845, Train Acc: 0.9704, Test Loss: 0.4203, Test Acc: 0.8854\n","Epoch [102/200], Train Loss: 0.0807, Train Acc: 0.9723, Test Loss: 0.4562, Test Acc: 0.8871\n","Epoch [103/200], Train Loss: 0.0837, Train Acc: 0.9727, Test Loss: 0.3883, Test Acc: 0.8919\n","Epoch [104/200], Train Loss: 0.0784, Train Acc: 0.9731, Test Loss: 0.4357, Test Acc: 0.8856\n","Epoch [105/200], Train Loss: 0.0742, Train Acc: 0.9743, Test Loss: 0.4288, Test Acc: 0.8842\n","Epoch [106/200], Train Loss: 0.0741, Train Acc: 0.9745, Test Loss: 0.4253, Test Acc: 0.8857\n","Epoch [107/200], Train Loss: 0.0843, Train Acc: 0.9706, Test Loss: 0.4088, Test Acc: 0.8955\n","Epoch [108/200], Train Loss: 0.0783, Train Acc: 0.9728, Test Loss: 0.4344, Test Acc: 0.8900\n","Epoch [109/200], Train Loss: 0.0738, Train Acc: 0.9749, Test Loss: 0.4636, Test Acc: 0.8797\n","Epoch [110/200], Train Loss: 0.0777, Train Acc: 0.9731, Test Loss: 0.4467, Test Acc: 0.8879\n","Epoch [111/200], Train Loss: 0.0771, Train Acc: 0.9734, Test Loss: 0.4665, Test Acc: 0.8836\n","Epoch [112/200], Train Loss: 0.0706, Train Acc: 0.9764, Test Loss: 0.4214, Test Acc: 0.8899\n","Epoch [113/200], Train Loss: 0.0749, Train Acc: 0.9748, Test Loss: 0.4352, Test Acc: 0.8877\n","Epoch [114/200], Train Loss: 0.0694, Train Acc: 0.9754, Test Loss: 0.4538, Test Acc: 0.8818\n","Epoch [115/200], Train Loss: 0.0754, Train Acc: 0.9749, Test Loss: 0.4122, Test Acc: 0.8942\n","Epoch [116/200], Train Loss: 0.0748, Train Acc: 0.9741, Test Loss: 0.4304, Test Acc: 0.8882\n","Epoch [117/200], Train Loss: 0.0799, Train Acc: 0.9725, Test Loss: 0.4113, Test Acc: 0.8965\n","Epoch [118/200], Train Loss: 0.0743, Train Acc: 0.9747, Test Loss: 0.4051, Test Acc: 0.8892\n","Epoch [119/200], Train Loss: 0.0732, Train Acc: 0.9759, Test Loss: 0.4632, Test Acc: 0.8798\n","Epoch [120/200], Train Loss: 0.0785, Train Acc: 0.9731, Test Loss: 0.4117, Test Acc: 0.8906\n","Epoch [121/200], Train Loss: 0.0721, Train Acc: 0.9755, Test Loss: 0.3920, Test Acc: 0.9000\n","Epoch [122/200], Train Loss: 0.0703, Train Acc: 0.9763, Test Loss: 0.3899, Test Acc: 0.8943\n","Epoch [123/200], Train Loss: 0.0731, Train Acc: 0.9752, Test Loss: 0.4785, Test Acc: 0.8758\n","Epoch [124/200], Train Loss: 0.0760, Train Acc: 0.9746, Test Loss: 0.4063, Test Acc: 0.8898\n","Epoch [125/200], Train Loss: 0.0680, Train Acc: 0.9771, Test Loss: 0.4347, Test Acc: 0.8899\n","Epoch [126/200], Train Loss: 0.0683, Train Acc: 0.9774, Test Loss: 0.4357, Test Acc: 0.8835\n","Epoch [127/200], Train Loss: 0.0716, Train Acc: 0.9759, Test Loss: 0.4164, Test Acc: 0.8924\n","Epoch [128/200], Train Loss: 0.0706, Train Acc: 0.9765, Test Loss: 0.4348, Test Acc: 0.8847\n","Epoch [129/200], Train Loss: 0.0690, Train Acc: 0.9766, Test Loss: 0.4308, Test Acc: 0.8908\n","Epoch [130/200], Train Loss: 0.0749, Train Acc: 0.9747, Test Loss: 0.4066, Test Acc: 0.8890\n","Epoch [131/200], Train Loss: 0.0702, Train Acc: 0.9756, Test Loss: 0.4086, Test Acc: 0.8923\n","Epoch [132/200], Train Loss: 0.0730, Train Acc: 0.9745, Test Loss: 0.3906, Test Acc: 0.8944\n","Epoch [133/200], Train Loss: 0.0666, Train Acc: 0.9777, Test Loss: 0.4488, Test Acc: 0.8910\n","Epoch [134/200], Train Loss: 0.0689, Train Acc: 0.9766, Test Loss: 0.3929, Test Acc: 0.8944\n","Epoch [135/200], Train Loss: 0.0684, Train Acc: 0.9770, Test Loss: 0.3838, Test Acc: 0.8962\n","Epoch [136/200], Train Loss: 0.0726, Train Acc: 0.9758, Test Loss: 0.4403, Test Acc: 0.8877\n","Epoch [137/200], Train Loss: 0.0672, Train Acc: 0.9769, Test Loss: 0.4621, Test Acc: 0.8877\n","Epoch [138/200], Train Loss: 0.0663, Train Acc: 0.9773, Test Loss: 0.4611, Test Acc: 0.8883\n","Epoch [139/200], Train Loss: 0.0737, Train Acc: 0.9756, Test Loss: 0.4307, Test Acc: 0.8870\n","Epoch [140/200], Train Loss: 0.0675, Train Acc: 0.9780, Test Loss: 0.4383, Test Acc: 0.8884\n","Epoch [141/200], Train Loss: 0.0668, Train Acc: 0.9782, Test Loss: 0.4058, Test Acc: 0.8951\n","Epoch [142/200], Train Loss: 0.0654, Train Acc: 0.9777, Test Loss: 0.4206, Test Acc: 0.8934\n","Epoch [143/200], Train Loss: 0.0665, Train Acc: 0.9771, Test Loss: 0.4102, Test Acc: 0.8978\n","Epoch [144/200], Train Loss: 0.0685, Train Acc: 0.9772, Test Loss: 0.4247, Test Acc: 0.8876\n","Epoch [145/200], Train Loss: 0.0708, Train Acc: 0.9760, Test Loss: 0.4256, Test Acc: 0.8941\n","Epoch [146/200], Train Loss: 0.0718, Train Acc: 0.9758, Test Loss: 0.4132, Test Acc: 0.8959\n","Epoch [147/200], Train Loss: 0.0718, Train Acc: 0.9751, Test Loss: 0.4234, Test Acc: 0.8932\n","Epoch [148/200], Train Loss: 0.0633, Train Acc: 0.9785, Test Loss: 0.4187, Test Acc: 0.8931\n","Epoch [149/200], Train Loss: 0.0720, Train Acc: 0.9765, Test Loss: 0.4715, Test Acc: 0.8846\n","Epoch [150/200], Train Loss: 0.0662, Train Acc: 0.9779, Test Loss: 0.4385, Test Acc: 0.8895\n","Epoch [151/200], Train Loss: 0.0640, Train Acc: 0.9779, Test Loss: 0.4097, Test Acc: 0.8918\n","Epoch [152/200], Train Loss: 0.0680, Train Acc: 0.9765, Test Loss: 0.4204, Test Acc: 0.8911\n","Epoch [153/200], Train Loss: 0.0698, Train Acc: 0.9764, Test Loss: 0.4007, Test Acc: 0.8937\n","Epoch [154/200], Train Loss: 0.0616, Train Acc: 0.9794, Test Loss: 0.4117, Test Acc: 0.8955\n","Epoch [155/200], Train Loss: 0.0646, Train Acc: 0.9781, Test Loss: 0.4334, Test Acc: 0.8946\n","Epoch [156/200], Train Loss: 0.0636, Train Acc: 0.9788, Test Loss: 0.4054, Test Acc: 0.8975\n","Epoch [157/200], Train Loss: 0.0678, Train Acc: 0.9770, Test Loss: 0.3956, Test Acc: 0.8948\n","Epoch [158/200], Train Loss: 0.0675, Train Acc: 0.9775, Test Loss: 0.3958, Test Acc: 0.8974\n","Epoch [159/200], Train Loss: 0.0644, Train Acc: 0.9781, Test Loss: 0.3853, Test Acc: 0.8988\n","Epoch [160/200], Train Loss: 0.0590, Train Acc: 0.9800, Test Loss: 0.4063, Test Acc: 0.8978\n","Epoch [161/200], Train Loss: 0.0675, Train Acc: 0.9776, Test Loss: 0.4202, Test Acc: 0.8969\n","Epoch [162/200], Train Loss: 0.0694, Train Acc: 0.9768, Test Loss: 0.3865, Test Acc: 0.8967\n","Epoch [163/200], Train Loss: 0.0612, Train Acc: 0.9797, Test Loss: 0.4193, Test Acc: 0.8916\n","Epoch [164/200], Train Loss: 0.0637, Train Acc: 0.9785, Test Loss: 0.4003, Test Acc: 0.8963\n","Epoch [165/200], Train Loss: 0.0633, Train Acc: 0.9789, Test Loss: 0.4254, Test Acc: 0.8908\n","Epoch [166/200], Train Loss: 0.0661, Train Acc: 0.9775, Test Loss: 0.4431, Test Acc: 0.8922\n","Epoch [167/200], Train Loss: 0.0670, Train Acc: 0.9771, Test Loss: 0.4244, Test Acc: 0.8948\n","Epoch [168/200], Train Loss: 0.0623, Train Acc: 0.9796, Test Loss: 0.4288, Test Acc: 0.8894\n","Epoch [169/200], Train Loss: 0.0623, Train Acc: 0.9791, Test Loss: 0.4040, Test Acc: 0.8966\n","Epoch [170/200], Train Loss: 0.0651, Train Acc: 0.9777, Test Loss: 0.4205, Test Acc: 0.8956\n","Epoch [171/200], Train Loss: 0.0673, Train Acc: 0.9782, Test Loss: 0.3878, Test Acc: 0.8918\n","Epoch [172/200], Train Loss: 0.0586, Train Acc: 0.9806, Test Loss: 0.3998, Test Acc: 0.8941\n","Epoch [173/200], Train Loss: 0.0703, Train Acc: 0.9757, Test Loss: 0.3940, Test Acc: 0.8962\n","Epoch [174/200], Train Loss: 0.0673, Train Acc: 0.9782, Test Loss: 0.4780, Test Acc: 0.8796\n","Epoch [175/200], Train Loss: 0.0632, Train Acc: 0.9795, Test Loss: 0.4046, Test Acc: 0.8945\n","Epoch [176/200], Train Loss: 0.0640, Train Acc: 0.9793, Test Loss: 0.4372, Test Acc: 0.8893\n","Epoch [177/200], Train Loss: 0.0670, Train Acc: 0.9777, Test Loss: 0.4088, Test Acc: 0.8927\n","Epoch [178/200], Train Loss: 0.0678, Train Acc: 0.9768, Test Loss: 0.4302, Test Acc: 0.8914\n","Epoch [179/200], Train Loss: 0.0711, Train Acc: 0.9762, Test Loss: 0.4023, Test Acc: 0.8960\n","Epoch [180/200], Train Loss: 0.0633, Train Acc: 0.9786, Test Loss: 0.4639, Test Acc: 0.8814\n","Epoch [181/200], Train Loss: 0.0636, Train Acc: 0.9779, Test Loss: 0.4106, Test Acc: 0.8943\n","Epoch [182/200], Train Loss: 0.0662, Train Acc: 0.9780, Test Loss: 0.3906, Test Acc: 0.8960\n","Epoch [183/200], Train Loss: 0.0577, Train Acc: 0.9810, Test Loss: 0.4369, Test Acc: 0.8979\n","Epoch [184/200], Train Loss: 0.0662, Train Acc: 0.9777, Test Loss: 0.4267, Test Acc: 0.8898\n","Epoch [185/200], Train Loss: 0.0609, Train Acc: 0.9797, Test Loss: 0.4089, Test Acc: 0.8962\n","Epoch [186/200], Train Loss: 0.0627, Train Acc: 0.9787, Test Loss: 0.4086, Test Acc: 0.8935\n","Epoch [187/200], Train Loss: 0.0615, Train Acc: 0.9798, Test Loss: 0.3646, Test Acc: 0.9012\n","Epoch [188/200], Train Loss: 0.0597, Train Acc: 0.9802, Test Loss: 0.4066, Test Acc: 0.8937\n","Epoch [189/200], Train Loss: 0.0597, Train Acc: 0.9800, Test Loss: 0.4317, Test Acc: 0.8861\n","Epoch [190/200], Train Loss: 0.0674, Train Acc: 0.9770, Test Loss: 0.3972, Test Acc: 0.8979\n","Epoch [191/200], Train Loss: 0.0623, Train Acc: 0.9789, Test Loss: 0.4396, Test Acc: 0.8892\n","Epoch [192/200], Train Loss: 0.0607, Train Acc: 0.9799, Test Loss: 0.4250, Test Acc: 0.8872\n","Epoch [193/200], Train Loss: 0.0625, Train Acc: 0.9794, Test Loss: 0.3960, Test Acc: 0.9001\n","Epoch [194/200], Train Loss: 0.0595, Train Acc: 0.9802, Test Loss: 0.4391, Test Acc: 0.8921\n","Epoch [195/200], Train Loss: 0.0645, Train Acc: 0.9785, Test Loss: 0.4644, Test Acc: 0.8808\n","Epoch [196/200], Train Loss: 0.0621, Train Acc: 0.9796, Test Loss: 0.4372, Test Acc: 0.8876\n","Epoch [197/200], Train Loss: 0.0609, Train Acc: 0.9793, Test Loss: 0.4318, Test Acc: 0.8962\n","Epoch [198/200], Train Loss: 0.0655, Train Acc: 0.9779, Test Loss: 0.4195, Test Acc: 0.8932\n","Epoch [199/200], Train Loss: 0.0651, Train Acc: 0.9786, Test Loss: 0.4327, Test Acc: 0.8891\n","Epoch [200/200], Train Loss: 0.0613, Train Acc: 0.9796, Test Loss: 0.4456, Test Acc: 0.8897\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","\n","# Set device to use\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define data transformations\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","# Load CIFAR10 dataset\n","trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","\n","# Define data loaders\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","# Define VGG16 model\n","model = models.vgg16(pretrained=False, num_classes=10).to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n","\n","# Train the model\n","num_epochs = 200\n","for epoch in range(num_epochs):\n","    # Train the model\n","    model.train()\n","    train_loss = 0\n","    train_correct = 0\n","    for images, labels in trainloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item() * images.size(0)\n","        train_correct += (outputs.argmax(dim=1) == labels).sum().item()\n","\n","    # Test the model\n","    model.eval()\n","    test_loss = 0\n","    test_correct = 0\n","    with torch.no_grad():\n","        for images, labels in testloader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            test_loss += loss.item() * images.size(0)\n","            test_correct += (outputs.argmax(dim=1) == labels).sum().item()\n","\n","    # Print results for this epoch\n","    train_loss /= len(trainset)\n","    train_acc = train_correct / len(trainset)\n","    test_loss /= len(testset)\n","    test_acc = test_correct / len(testset)\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n","\n","# Save the trained model\n","torch.save(model.state_dict(), 'vgg16_cifar10.pth')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
